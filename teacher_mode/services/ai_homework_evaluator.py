"""
–°–µ—Ä–≤–∏—Å –¥–ª—è AI –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤ —É—á–µ–Ω–∏–∫–æ–≤ –≤ –¥–æ–º–∞—à–Ω–∏—Ö –∑–∞–¥–∞–Ω–∏—è—Ö.
"""

import logging
from typing import Optional, Dict, Tuple

logger = logging.getLogger(__name__)


async def evaluate_homework_answer(
    task_module: str,
    question_data: Dict,
    user_answer: str,
    user_id: int
) -> Tuple[bool, str]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ç–≤–µ—Ç —É—á–µ–Ω–∏–∫–∞ —á–µ—Ä–µ–∑ AI evaluator —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –º–æ–¥—É–ª—è.

    Args:
        task_module: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è ('task19', 'task20', 'task24', 'task25', 'test_part', 'custom')
        question_data: –î–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å–∞ –∏–∑ question_loader –∏–ª–∏ custom_questions
        user_answer: –û—Ç–≤–µ—Ç —É—á–µ–Ω–∏–∫–∞
        user_id: ID —É—á–µ–Ω–∏–∫–∞

    Returns:
        Tuple[bool, str]: (is_correct, feedback_text)
        - is_correct: True –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø—Ä–∏–Ω—è—Ç (–Ω–∞–±—Ä–∞–Ω–æ > 50% –±–∞–ª–ª–æ–≤)
        - feedback_text: –¢–µ–∫—Å—Ç –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –¥–ª—è —É—á–µ–Ω–∏–∫–∞
    """
    try:
        # –î–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–∏–ø
        if task_module == 'custom':
            custom_type = question_data.get('type', 'test_part')
            return await _evaluate_custom_question(custom_type, question_data, user_answer, user_id)

        if task_module == 'test_part':
            return await _evaluate_test_part(question_data, user_answer, user_id)
        elif task_module == 'task19':
            return await _evaluate_task19(question_data, user_answer, user_id)
        elif task_module == 'task20':
            return await _evaluate_task20(question_data, user_answer, user_id)
        elif task_module == 'task24':
            return await _evaluate_task24(question_data, user_answer, user_id)
        elif task_module == 'task25':
            return await _evaluate_task25(question_data, user_answer, user_id)
        else:
            logger.warning(f"Unknown task module: {task_module}")
            return False, f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∑–∞–¥–∞–Ω–∏—è: {task_module}"

    except Exception as e:
        logger.error(f"Error evaluating answer for {task_module}: {e}", exc_info=True)
        return False, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ—Ç–≤–µ—Ç–∞: {str(e)}"


async def _evaluate_task19(question_data: Dict, user_answer: str, user_id: int) -> Tuple[bool, str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è task19 (–ø—Ä–∏–º–µ—Ä—ã —Å –æ–±—â–µ—Å—Ç–≤–æ–∑–Ω–∞–Ω–∏–µ–º)"""
    try:
        from task19.evaluator import Task19AIEvaluator, StrictnessLevel
        from core.types import EvaluationResult

        # –°–æ–∑–¥–∞–µ–º evaluator
        evaluator = Task19AIEvaluator(strictness=StrictnessLevel.STANDARD)

        # –í—ã–∑—ã–≤–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
        topic = question_data.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ç–µ–º–∞')
        task_text = question_data.get('task_text', '')

        result: EvaluationResult = await evaluator.evaluate(
            answer=user_answer,
            topic=topic,
            task_text=task_text
        )

        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å
        is_correct = result.total_score >= (result.max_score / 2)  # >= 50% –±–∞–ª–ª–æ–≤

        feedback = f"üìä <b>–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏:</b>\n\n"
        feedback += f"–ë–∞–ª–ª—ã: {result.total_score}/{result.max_score}\n\n"
        feedback += f"<b>–û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å:</b>\n{result.feedback}"

        if result.warnings:
            feedback += f"\n\n‚ö†Ô∏è <b>–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:</b>\n"
            feedback += "\n".join(f"‚Ä¢ {w}" for w in result.warnings)

        if result.suggestions:
            feedback += f"\n\nüí° <b>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:</b>\n"
            feedback += "\n".join(f"‚Ä¢ {s}" for s in result.suggestions)

        return is_correct, feedback

    except ImportError as e:
        logger.warning(f"Task19 evaluator not available: {e}")
        return True, "‚úÖ –û—Ç–≤–µ—Ç –ø—Ä–∏–Ω—è—Ç (AI –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)"
    except Exception as e:
        logger.error(f"Error in task19 evaluation: {e}", exc_info=True)
        return False, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ: {str(e)}"


async def _evaluate_task20(question_data: Dict, user_answer: str, user_id: int) -> Tuple[bool, str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è task20 (–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏)"""
    try:
        from task20.evaluator import Task20AIEvaluator
        from core.types import EvaluationResult

        # –°–æ–∑–¥–∞–µ–º evaluator
        evaluator = Task20AIEvaluator()

        # –í—ã–∑—ã–≤–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
        topic = question_data.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ç–µ–º–∞')
        task_text = question_data.get('task_text', '')

        result: EvaluationResult = await evaluator.evaluate(
            answer=user_answer,
            topic=topic,
            task_text=task_text
        )

        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å
        is_correct = result.total_score >= (result.max_score / 2)

        feedback = f"üìä <b>–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏:</b>\n\n"
        feedback += f"–ë–∞–ª–ª—ã: {result.total_score}/{result.max_score}\n\n"
        feedback += f"<b>–û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å:</b>\n{result.feedback}"

        if result.warnings:
            feedback += f"\n\n‚ö†Ô∏è <b>–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:</b>\n"
            feedback += "\n".join(f"‚Ä¢ {w}" for w in result.warnings)

        if result.suggestions:
            feedback += f"\n\nüí° <b>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:</b>\n"
            feedback += "\n".join(f"‚Ä¢ {s}" for s in result.suggestions)

        return is_correct, feedback

    except ImportError as e:
        logger.warning(f"Task20 evaluator not available: {e}")
        return True, "‚úÖ –û—Ç–≤–µ—Ç –ø—Ä–∏–Ω—è—Ç (AI –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)"
    except Exception as e:
        logger.error(f"Error in task20 evaluation: {e}", exc_info=True)
        return False, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ: {str(e)}"


async def _evaluate_task24(question_data: Dict, user_answer: str, user_id: int) -> Tuple[bool, str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è task24 (—Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –ø–ª–∞–Ω)"""
    try:
        from task24.checker import evaluate_plan_with_ai
        from task24.handlers import bot_data  # –ì–ª–æ–±–∞–ª—å–Ω—ã–π –æ–±—ä–µ–∫—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–ª–∞–Ω–æ–≤

        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –ø–ª–∞–Ω–∞
        topic_name = question_data.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ç–µ–º–∞')

        # –§–æ—Ä–º–∏—Ä—É–µ–º ideal_plan_data –∏–∑ question_data
        ideal_plan_data = {
            'full_plan': question_data.get('full_plan', []),
            'points_data': question_data.get('points_data', []),
            'min_points': question_data.get('min_points', 3),
            'min_detailed_points': question_data.get('min_detailed_points', 2),
            'min_subpoints': question_data.get('min_subpoints', 3)
        }

        # –í—ã–∑—ã–≤–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
        feedback_text = await evaluate_plan_with_ai(
            user_plan_text=user_answer,
            ideal_plan_data=ideal_plan_data,
            bot_data=bot_data,
            topic_name=topic_name,
            use_ai=True,
            user_id=user_id
        )

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–ª–ª—ã –∏–∑ feedback
        import re
        k1_match = re.search(r'–ö1.*?(\d+)/3', feedback_text)
        k2_match = re.search(r'–ö2.*?(\d+)/1', feedback_text)
        k1 = int(k1_match.group(1)) if k1_match else 0
        k2 = int(k2_match.group(1)) if k2_match else 0

        total_score = k1 + k2
        max_score = 4

        is_correct = total_score >= (max_score / 2)  # >= 2 –±–∞–ª–ª–æ–≤ –∏–∑ 4

        feedback = f"üìä <b>–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–ª–∞–Ω–∞:</b>\n\n"
        feedback += feedback_text

        return is_correct, feedback

    except ImportError as e:
        logger.warning(f"Task24 checker not available: {e}")
        return True, "‚úÖ –û—Ç–≤–µ—Ç –ø—Ä–∏–Ω—è—Ç (AI –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)"
    except Exception as e:
        logger.error(f"Error in task24 evaluation: {e}", exc_info=True)
        return False, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ: {str(e)}"


async def _evaluate_task25(question_data: Dict, user_answer: str, user_id: int) -> Tuple[bool, str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è task25 (—ç—Å—Å–µ)"""
    try:
        from task25.evaluator import Task25AIEvaluator
        from core.types import EvaluationResult

        # –°–æ–∑–¥–∞–µ–º evaluator
        evaluator = Task25AIEvaluator()

        # –í—ã–∑—ã–≤–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
        result: EvaluationResult = await evaluator.evaluate(
            answer=user_answer,
            topic=question_data,  # –ü–µ—Ä–µ–¥–∞–µ–º –≤–µ—Å—å question_data –∫–∞–∫ topic
            user_id=user_id
        )

        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å
        is_correct = result.total_score >= (result.max_score / 2)

        feedback = f"üìä <b>–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏:</b>\n\n"
        feedback += f"–ë–∞–ª–ª—ã: {result.total_score}/{result.max_score}\n\n"

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª–∏ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º
        if result.criteria_scores:
            feedback += "<b>–ü–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º:</b>\n"
            for criterion, score in result.criteria_scores.items():
                feedback += f"‚Ä¢ {criterion}: {score}\n"
            feedback += "\n"

        feedback += f"<b>–û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å:</b>\n{result.feedback}"

        if result.warnings:
            feedback += f"\n\n‚ö†Ô∏è <b>–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:</b>\n"
            feedback += "\n".join(f"‚Ä¢ {w}" for w in result.warnings)

        if result.suggestions:
            feedback += f"\n\nüí° <b>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:</b>\n"
            feedback += "\n".join(f"‚Ä¢ {s}" for s in result.suggestions)

        return is_correct, feedback

    except ImportError as e:
        logger.warning(f"Task25 evaluator not available: {e}")
        return True, "‚úÖ –û—Ç–≤–µ—Ç –ø—Ä–∏–Ω—è—Ç (AI –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)"
    except Exception as e:
        logger.error(f"Error in task25 evaluation: {e}", exc_info=True)
        return False, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ: {str(e)}"


async def _evaluate_test_part(question_data: Dict, user_answer: str, user_id: int) -> Tuple[bool, str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π —á–∞—Å—Ç–∏ (–∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç)"""
    try:
        # –î–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π —á–∞—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—á–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        correct_answer = question_data.get('answer', question_data.get('correct_answer', ''))

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç–≤–µ—Ç—ã (—É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É)
        user_answer_normalized = user_answer.strip().lower().replace(' ', '')
        correct_answer_normalized = str(correct_answer).strip().lower().replace(' ', '')

        is_correct = user_answer_normalized == correct_answer_normalized

        if is_correct:
            feedback = (
                "‚úÖ <b>–û—Ç–≤–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π!</b>\n\n"
                f"–í–∞—à –æ—Ç–≤–µ—Ç: <code>{user_answer}</code>\n"
                f"–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: <code>{correct_answer}</code>"
            )
        else:
            feedback = (
                "‚ùå <b>–û—Ç–≤–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π</b>\n\n"
                f"–í–∞—à –æ—Ç–≤–µ—Ç: <code>{user_answer}</code>\n"
                f"–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: <code>{correct_answer}</code>"
            )

        return is_correct, feedback

    except Exception as e:
        logger.error(f"Error in test_part evaluation: {e}", exc_info=True)
        return False, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ: {str(e)}"


async def _evaluate_custom_question(
    custom_type: str,
    question_data: Dict,
    user_answer: str,
    user_id: int
) -> Tuple[bool, str]:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º evaluator —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ç–∏–ø–∞.

    Args:
        custom_type: –¢–∏–ø –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ ('test_part', 'task19', 'task20', 'task24', 'task25')
        question_data: –î–∞–Ω–Ω—ã–µ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ (–≤–∫–ª—é—á–∞—è text, type, correct_answer)
        user_answer: –û—Ç–≤–µ—Ç —É—á–µ–Ω–∏–∫–∞
        user_id: ID —É—á–µ–Ω–∏–∫–∞

    Returns:
        Tuple[bool, str]: (is_correct, feedback_text)
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞ –∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç/–∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏–∑ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
        question_text = question_data.get('text', '')
        correct_answer = question_data.get('correct_answer')

        # –î–ª—è test_part –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
        if custom_type == 'test_part':
            if correct_answer:
                # –ï—Å–ª–∏ —É—á–∏—Ç–µ–ª—å —É–∫–∞–∑–∞–ª –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º
                user_answer_normalized = user_answer.strip().lower().replace(' ', '')
                correct_answer_normalized = str(correct_answer).strip().lower().replace(' ', '')

                is_correct = user_answer_normalized == correct_answer_normalized

                if is_correct:
                    feedback = (
                        "‚úÖ <b>–û—Ç–≤–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π!</b>\n\n"
                        f"–í–∞—à –æ—Ç–≤–µ—Ç: <code>{user_answer}</code>\n"
                        f"–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: <code>{correct_answer}</code>"
                    )
                else:
                    feedback = (
                        "‚ùå <b>–û—Ç–≤–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π</b>\n\n"
                        f"–í–∞—à –æ—Ç–≤–µ—Ç: <code>{user_answer}</code>\n"
                        f"–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: <code>{correct_answer}</code>"
                    )

                return is_correct, feedback
            else:
                # –ï—Å–ª–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–µ —É–∫–∞–∑–∞–Ω, –ø—Ä–∏–Ω–∏–º–∞–µ–º –æ—Ç–≤–µ—Ç –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏
                feedback = (
                    "‚úÖ <b>–û—Ç–≤–µ—Ç –ø—Ä–∏–Ω—è—Ç</b>\n\n"
                    f"–í–∞—à –æ—Ç–≤–µ—Ç: <code>{user_answer}</code>\n\n"
                    "üí° –£—á–∏—Ç–µ–ª—å –Ω–µ —É–∫–∞–∑–∞–ª –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, –ø–æ—ç—Ç–æ–º—É –≤–∞—à –æ—Ç–≤–µ—Ç –ø—Ä–∏–Ω—è—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏."
                )
                return True, feedback

        # –î–ª—è –∑–∞–¥–∞–Ω–∏–π 19, 20, 24, 25 –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ AI evaluators
        elif custom_type == 'task19':
            # –§–æ—Ä–º–∏—Ä—É–µ–º question_data –¥–ª—è evaluator
            eval_question_data = {
                'title': '–ö–∞—Å—Ç–æ–º–Ω—ã–π –≤–æ–ø—Ä–æ—Å',
                'task_text': question_text
            }

            # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã –∫—Ä–∏—Ç–µ—Ä–∏–∏, –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö –≤ feedback
            if correct_answer:
                eval_question_data['criteria'] = correct_answer

            return await _evaluate_task19(eval_question_data, user_answer, user_id)

        elif custom_type == 'task20':
            eval_question_data = {
                'title': '–ö–∞—Å—Ç–æ–º–Ω—ã–π –≤–æ–ø—Ä–æ—Å',
                'task_text': question_text
            }

            if correct_answer:
                eval_question_data['criteria'] = correct_answer

            return await _evaluate_task20(eval_question_data, user_answer, user_id)

        elif custom_type == 'task24':
            eval_question_data = {
                'topic': '–ö–∞—Å—Ç–æ–º–Ω—ã–π –ø–ª–∞–Ω',
                'full_plan': [],
                'points_data': [],
                'min_points': 3,
                'min_detailed_points': 2,
                'min_subpoints': 3
            }

            if correct_answer:
                # –ï—Å–ª–∏ —É—á–∏—Ç–µ–ª—å —É–∫–∞–∑–∞–ª –∫—Ä–∏—Ç–µ—Ä–∏–∏, –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö
                eval_question_data['description'] = f"–ö—Ä–∏—Ç–µ—Ä–∏–∏: {correct_answer}"

            return await _evaluate_task24(eval_question_data, user_answer, user_id)

        elif custom_type == 'task25':
            eval_question_data = {
                'title': '–ö–∞—Å—Ç–æ–º–Ω–æ–µ —ç—Å—Å–µ',
                'task_text': question_text
            }

            if correct_answer:
                eval_question_data['criteria'] = correct_answer

            return await _evaluate_task25(eval_question_data, user_answer, user_id)

        else:
            logger.warning(f"Unknown custom question type: {custom_type}")
            return True, f"‚úÖ –û—Ç–≤–µ—Ç –ø—Ä–∏–Ω—è—Ç (–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∑–∞–¥–∞–Ω–∏—è: {custom_type})"

    except Exception as e:
        logger.error(f"Error evaluating custom question: {e}", exc_info=True)
        return False, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞: {str(e)}"
